---
title: "Principal Component Analysis"
output:
  pdf_document: default
  html_notebook: default
---

When faced with a large set of correlated variables, principal components allow us to summarize this set with a smaller number of representative variables that collectively explain most of the variability in the original set.

The principal component directions are directions in feature space along which the original data are highly variable.

Principal component analysis (PCA) refers to the process by which principal components are computed, and the subsequent use of these compo- nents in understanding the data.

PCA is an unsupervised approach, since it involves only a set of features X1, X2, . . . , Xp, and no associated response Y . Apart from producing derived variables for use in supervised learning problems, PCA also serves as a tool for data visualization.


Here, we perform PCA on the USArrests data set, which is part of the base R package. The rows of the data set contain the 50 states, in alphabetical order.

```{r}
data("USArrests")
str(USArrests)
```
lets compute the mean and variance of each variable.
```{r}
apply(USArrests , 2, mean)
apply(USArrests,2,var)
```
We see that there are on average three times as many rapes as murders, and more than eight times as many assaults as rapes.the variables also have vastly different variances.

The UrbanPop variable measures the percentage of the population in each state living in an urban area, which is not a comparable number to the number of rapes in each state per 100,000 individuals. 

If we failed to scale the variables before performing PCA, then most of the principal components that we observed would be driven by the Assault variable, since it has by far the largest mean and variance. 

Thus, it is important to standardize the variables to have mean zero and standard deviation one before performing PCA. We now perform principal components analysis using the prcomp() function.
```{r}
pr.out=prcomp(USArrests, scale=TRUE)
names(pr.out)
```

The center and scale components correspond to the means and standard deviations of the variables that were used for scaling prior to implementing PCA.
```{r}
pr.out$center
pr.out$scale
```
The rotation matrix provides the principal component loadings; each column of pr.out$rotation contains the corresponding principal component loading vector.
```{r}
pr.out$rotation
```
We see that there are four distinct principal components. In general min(n âˆ’ 1, p) informative principal components in a data set with n observations and p variables.

Using the prcomp() function, we do not need to explicitly multiply the data by the principal component loading vectors in order to obtain the principal component score vectors. Matrix x has as its columns the principal component score vectors. That is, the kth column is the kth principal component score vector.

We can plot the first two principal components as follows
```{r}
biplot(pr.out, scale=0)
```

```{r}
pr.out$rotation=-pr.out$rotation 
pr.out$x=-pr.out$x
biplot(pr.out, scale=0)

```
The prcomp() function also outputs the standard deviation of each principal component.
```{r}
pr.out$sdev
pr.var=pr.out$sdev ^2 #variance explained by each principal component
pr.var
```
To compute the proportion of variance explained by each principal component, we simply divide the variance explained by each principal component by the total variance explained by all four principal components:
```{r}
pve=pr.var/sum(pr.var)
pve
```
We see that the first principal component explains 62.0 % of the variance in the data, the next principal component explains 24.7 % of the variance, and so forth.

We can plot the PVE explained by each component, as well as the cumulative PVE.
```{r}
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained ", ylim=c(0,1),type='b')
plot(cumsum(pve), xlab="Principal Component ", ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), type='b')
```

